{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Hiding Workshop Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to use the model to detect faces in a video. Fill in the gaps in the code blocks below. For more information about the OpenVINO Inference Engine Python API, see the [official documentation](https://docs.openvinotoolkit.org/latest/ie_python_api/annotated.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0. Preparation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we need to install requirements fo this workshop.\n",
    "We prepared a specific package to process inference results of RetinaFace. In addition we need packages like numpy to work with tensors and IPython to show a video in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting IPython==7.5.0\n",
      "  Downloading ipython-7.5.0-py3-none-any.whl (770 kB)\n",
      "\u001b[K     |████████████████████████████████| 770 kB 6.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting matplotlib==3.3.4\n",
      "  Using cached matplotlib-3.3.4-cp36-cp36m-manylinux1_x86_64.whl (11.5 MB)\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.6/dist-packages/PyYAML-5.4.1-py3.6-linux-x86_64.egg (from -r requirements.txt (line 4)) (5.4.1)\n",
      "Collecting requests>=2.25.1\n",
      "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 732 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from IPython==7.5.0->-r requirements.txt (line 1)) (0.7.5)\n",
      "Requirement already satisfied: pexpect in /usr/local/lib/python3.6/dist-packages (from IPython==7.5.0->-r requirements.txt (line 1)) (4.8.0)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.6/dist-packages (from IPython==7.5.0->-r requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.6/dist-packages (from IPython==7.5.0->-r requirements.txt (line 1)) (0.18.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from IPython==7.5.0->-r requirements.txt (line 1)) (4.4.2)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from IPython==7.5.0->-r requirements.txt (line 1)) (4.3.3)\n",
      "Collecting prompt-toolkit<2.1.0,>=2.0.0\n",
      "  Downloading prompt_toolkit-2.0.10-py3-none-any.whl (340 kB)\n",
      "\u001b[K     |████████████████████████████████| 340 kB 13.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from IPython==7.5.0->-r requirements.txt (line 1)) (57.0.0)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from IPython==7.5.0->-r requirements.txt (line 1)) (2.9.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.3.4->-r requirements.txt (line 2)) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/u44598/.local/lib/python3.6/site-packages (from matplotlib==3.3.4->-r requirements.txt (line 2)) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.3.4->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.3.4->-r requirements.txt (line 2)) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/u44598/.local/lib/python3.6/site-packages (from matplotlib==3.3.4->-r requirements.txt (line 2)) (1.16.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.3.4->-r requirements.txt (line 2)) (8.2.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/u44598/.local/lib/python3.6/site-packages (from requests>=2.25.1->-r requirements.txt (line 5)) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/u44598/.local/lib/python3.6/site-packages (from requests>=2.25.1->-r requirements.txt (line 5)) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/u44598/.local/lib/python3.6/site-packages (from requests>=2.25.1->-r requirements.txt (line 5)) (2.10)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Downloading charset_normalizer-2.0.4-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: six in /home/u44598/.local/lib/python3.6/site-packages (from cycler>=0.10->matplotlib==3.3.4->-r requirements.txt (line 2)) (1.15.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from jedi>=0.10->IPython==7.5.0->-r requirements.txt (line 1)) (0.8.2)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->IPython==7.5.0->-r requirements.txt (line 1)) (0.2.5)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->IPython==7.5.0->-r requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect->IPython==7.5.0->-r requirements.txt (line 1)) (0.7.0)\n",
      "Installing collected packages: prompt-toolkit, charset-normalizer, requests, matplotlib, IPython\n",
      "\u001b[33m  WARNING: The script normalizer is installed in '/home/u44598/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.24.0\n",
      "    Uninstalling requests-2.24.0:\n",
      "      Successfully uninstalled requests-2.24.0\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.3.0\n",
      "    Uninstalling matplotlib-3.3.0:\n",
      "      Successfully uninstalled matplotlib-3.3.0\n",
      "\u001b[33m  WARNING: The scripts iptest, iptest3, ipython and ipython3 are installed in '/home/u44598/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed IPython-7.5.0 charset-normalizer-2.0.4 matplotlib-3.3.4 prompt-toolkit-2.0.10 requests-2.26.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step of preparation is set some constants. This is paths to input and result videos and the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Contains all data for the workshop\n",
    "WORKSHOP_MODEL_PATH = Path('./data') / 'models'\n",
    "\n",
    "# Path to the Inference Engine model\n",
    "# But you can use the INT8 model instead\n",
    "FACE_DETECTION_MODEL_PATH_XML = WORKSHOP_MODEL_PATH / 'face-detection-adas-0001.xml'\n",
    "FACE_DETECTION_MODEL_PATH_BIN = WORKSHOP_MODEL_PATH / 'face-detection-adas-0001.bin'\n",
    "\n",
    "DEVICE = 'CPU'\n",
    "\n",
    "DATA_PATH = Path('./data')\n",
    "INPUT_VIDEO = str(DATA_PATH / 'input.mp4')\n",
    "OUTPUT_VIDEO = str(DATA_PATH / 'output.MP4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's show the input video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"600\" height=\"400\" controls><source src=\"data/input.mp4\" type=\"video/mp4\"></video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "# Show a source video\n",
    "HTML(f\"\"\"<video width=\"600\" height=\"400\" controls><source src=\"{INPUT_VIDEO}\" type=\"video/mp4\"></video>\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import OpenCV for work with a video and images\n",
    "import cv2\n",
    "\n",
    "# Import the Inference Engine\n",
    "from openvino.inference_engine import IECore, IENetwork\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first our function is to create output video writer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prapare_out_video_stream(input_video_stream: cv2.VideoCapture, output_video_file_path: str) -> cv2.VideoWriter:\n",
    "    width  = int(input_video_stream.get(3))\n",
    "    height = int(input_video_stream.get(4))\n",
    "    video_writer = cv2.VideoWriter(output_video_file_path, cv2.VideoWriter_fourcc(*'avc1'), 20, (width, height))\n",
    "    return video_writer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create an instance of the OpenVINO Inference Engine `IECore` class\n",
    "This class represents an Inference Engine entity \n",
    "and allows you to manipulate plugins using unified interfaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ie_core = IECore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/u44598/face-hiding-workshop/notebook\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Read the prepared model\n",
    "\n",
    "You need to create an instance of the IENetwork class.\n",
    "A constructor of this class has two parameters: \n",
    " 1. path to the .xml file of the model \n",
    " 2. path to the .bin file of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection_network = ie_core.read_network(FACE_DETECTION_MODEL_PATH_XML, FACE_DETECTION_MODEL_PATH_BIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Get the name of the input layer of the model\n",
    "\n",
    "To infer a model, you need to know input layers of the model\n",
    "The object `retinaface_network` contains information about inputs of the network in a property `input_info`,\n",
    "which is a dictionary: key - name of the input layer, volume - representation of the input network.\n",
    "In this case, you need to get the name and the blob of the input .`retinaface_input_name` should be a string, `retinaface_input_blob`  should be a `DataPtr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input layer of the RetinaFace is data\n"
     ]
    }
   ],
   "source": [
    "face_detection_input_name = next(iter(face_detection_network.input_info))\n",
    "face_detection_input_blob = face_detection_network.input_info[face_detection_input_name].input_data\n",
    "\n",
    "print(f'Input layer of the RetinaFace is {face_detection_input_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Get shape (dimensions) of the input layer of the network\n",
    "\n",
    "* n - number of batches\n",
    "* c - number of input image channels (usualy 3 - R, G and B) \n",
    "* h - height\n",
    "* w - width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape of the face-detection-adas-0001 is [1, 3, 384, 672]\n"
     ]
    }
   ],
   "source": [
    "face_detection_batch, face_detection_channels, face_detection_input_layer_h, face_detection_input_layer_w = face_detection_input_blob.shape\n",
    "\n",
    "print(f'Input shape of the face-detection-adas-0001 is [{face_detection_batch}, {face_detection_channels}, {face_detection_input_layer_h}, {face_detection_input_layer_w}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection_blob = next(iter(face_detection_network.outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Load the network to a device\n",
    "\n",
    "Use the instance of `IECore`.\n",
    "The class `IECore` has a special function called `load_network`, which loads a network to a device.\n",
    "This function prepares the network for the first inference on the device \n",
    "and returns an instance of the network prepared for an inference (execution). \n",
    "This function has many parameters, but in this case, you need to know only about two of them:\n",
    "* `network` - instance of `IENetwork`\n",
    "* `device_name` - string, contains a device name to infer a model on: CPU, GPU and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection_loaded_to_device = ie_core.load_network(face_detection_network, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Open the input video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video_stream = cv2.VideoCapture(INPUT_VIDEO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: PreProcessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_detection_pre_processing(input_frame: np.ndarray, batch: int, channels: int, input_layer_height: int, input_layer_width: int) -> np.ndarray:\n",
    "    # Resize the frame to the network input \n",
    "    resized_frame = cv2.resize(input_frame, (input_layer_width, input_layer_height))\n",
    "    \n",
    "    # Change the data layout from HWC to CHW\n",
    "    transposed_frame = resized_frame.transpose((2, 0, 1))  \n",
    "    \n",
    "    # Reshape the frame to the network input \n",
    "    reshaped_frame = transposed_frame.reshape((batch, channels, input_layer_height, input_layer_width))\n",
    "    \n",
    "    return reshaped_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_detection_inference(input_frame: np.ndarray) -> np.ndarray:\n",
    "    feed_dict = {\n",
    "        face_detection_input_name: input_frame\n",
    "    }\n",
    "    \n",
    "    # All is ready for the main thing - inference!\n",
    "    # You have read and loaded the network to the device, prepared input data and now you are ready to infer.\n",
    "    \n",
    "    # Step 11:\n",
    "    # To start an inference, call the `infer` function of the `network_loaded_to_device` variable. \n",
    "    # We must set input data (a dictionary).\n",
    "    inference_result = face_detection_loaded_to_device.infer(feed_dict)\n",
    "    \n",
    "    # Great! The `inference_result` variable contains output data after inference of the network.\n",
    "    # `inference_result` is a dictionary, \n",
    "    #  where key is the name of the output name, \n",
    "    #        value is data from the blob.\n",
    "    \n",
    "    return inference_result[face_detection_blob]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Prepare for post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Output video stream\n",
    "output_video_stream = prapare_out_video_stream(input_video_stream, OUTPUT_VIDEO)\n",
    "\n",
    "# Get input height and width\n",
    "input_frame_width = int(input_video_stream.get(3))   # float `width`\n",
    "input_frame_height = int(input_video_stream.get(4))  # float `height`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Function for processing inference results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_face_detection_inference_result_in_frame(original_frame: np.ndarray, detected_face: np.ndarray):       \n",
    "\n",
    "    # Step 14: Draw bounding boxes\n",
    "    # Draw a bounding box only for objects the confidence of which is greater than a specified threshold\n",
    "    # Get coordinates of a discovered object\n",
    "   \n",
    "    # Step 13: Get the confidence for a discovered object\n",
    "    confidence =  detected_face[2]\n",
    "        \n",
    "    if confidence < 0.5:\n",
    "        return\n",
    "    \n",
    "    frame_h, frame_w = original_frame.shape[:2]\n",
    "    \n",
    "    xmin = int(detected_face[3]*frame_w)\n",
    "    ymin = int(detected_face[4]*frame_h)\n",
    "\n",
    "    xmax = int(detected_face[5]*frame_w)\n",
    "    ymax = int(detected_face[6]*frame_h)\n",
    "    \n",
    "    w = xmax - xmin\n",
    "    h = ymax - ymin\n",
    "    \n",
    "    face = original_frame[ymin:ymax, xmin:xmax]\n",
    "    blured_face = cv2.GaussianBlur(face,(23, 23), 50)    \n",
    "    original_frame[ymin:ymax, xmin:xmax] = blured_face\n",
    "    \n",
    "    \n",
    "    # Get confidence for a discovered object\n",
    "    confidence = round(confidence * 100, 1)\n",
    "    \n",
    "    # Draw a box and a label\n",
    "    color = (0, 255, 0)\n",
    "    \n",
    "    # Create the title of an object\n",
    "    text = f'{confidence}%'\n",
    "\n",
    "    # Put the title to a frame\n",
    "    cv2.putText(original_frame, text, (xmin, ymin - 7), cv2.FONT_HERSHEY_COMPLEX, 2, color, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Loop over frames in the input video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "while input_video_stream.isOpened():\n",
    "    # Read the next frame from the intput video \n",
    "    ret, frame = input_video_stream.read()\n",
    "    # Check if the video is over\n",
    "    if not ret:\n",
    "        # Exit from the loop if the video is over\n",
    "        break \n",
    "    \n",
    "    # Prepare frame for inference\n",
    "    in_frame = face_detection_pre_processing(frame, face_detection_batch, face_detection_channels, face_detection_input_layer_h, face_detection_input_layer_w)\n",
    "    \n",
    "    \n",
    "    inference_result = face_detection_inference(in_frame)\n",
    "    \n",
    "    for detected_face in inference_result[0][0]:\n",
    "        add_face_detection_inference_result_in_frame(frame, detected_face)\n",
    "    \n",
    "    # Write the resulting frame to the output stream\n",
    "    output_video_stream.write(frame)\n",
    "    \n",
    "input_video_stream.release()\n",
    "# Save the resulting video\n",
    "output_video_stream.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"600\" height=\"400\" controls><source src=\"data/output.MP4\" type=\"video/mp4\"></video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "# Show a source video\n",
    "HTML(f\"\"\"<video width=\"600\" height=\"400\" controls><source src=\"{OUTPUT_VIDEO}\" type=\"video/mp4\"></video>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see boxes in the video? \n",
    "If yes, you did all right!\n",
    "**Good Work!** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 16: Practice (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the next step? Often from neural networks build pipelines. It is to use the results of the first neural network as an input for the next neural network. \n",
    "Let's try to build a pipeline from two networks:  first is finds a person on the video and the next to recognize the emotions of this person\n",
    "\n",
    "We have already run the first network. And find the person on the video.\n",
    "The next step is to find a network for emotion recognition.\n",
    "There is a good neural network in the [OpenModelZOO](https://docs.openvinotoolkit.org/2019_R1/_docs_Pre_Trained_Models.html) - [emotions-recognition-retail-0003 network](https://docs.openvinotoolkit.org/2019_R1/_emotions_recognition_retail_0003_description_emotions_recognition_retail_0003.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Download emotions-recognition-retail-0003 network\n",
    "Run the Model Downloader eith needed arguments to download the emotions-recognition-retail-0003 network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################|| Downloading emotions-recognition-retail-0003 ||################\n",
      "\n",
      "========== Downloading data/model/intel/emotions-recognition-retail-0003/FP16/emotions-recognition-retail-0003.xml\n",
      "... 100%, 39 KB, 291 KB/s, 0 seconds passed\n",
      "\n",
      "========== Downloading data/model/intel/emotions-recognition-retail-0003/FP16/emotions-recognition-retail-0003.bin\n",
      "... 100%, 4848 KB, 4297 KB/s, 1 seconds passed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 /opt/intel/openvino_2021/deployment_tools/open_model_zoo/tools/downloader/downloader.py --name emotions-recognition-retail-0003 --precision FP16 --output_dir data/model\n",
    "!mv data/model/intel/emotions-recognition-retail-0003/FP16/emotions-recognition-retail-0003.* data/models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This mode already is in OpenVINO format and you do not need to convert it.\n",
    "\n",
    "After downloading the model you can use it:\n",
    "\n",
    "### Step 2: Read the prepared model\n",
    "The IENetwork class is designed to work with a model in the Inference Engine. This class contains information about the network model read from the Intermediate Representation and allows you to manipulate some model parameters such as layers affinity and output layers.\n",
    "\n",
    "You need to create an instance of the IENetwork class. A constructor of this class has two parameters:\n",
    "\n",
    "path to the .xml file of the model\n",
    "path to the .bin file of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_recognition_network = ie_core.read_network('data/models/emotions-recognition-retail-0003.xml', 'data/models/emotions-recognition-retail-0003.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load the network to a device\n",
    "\n",
    "Use the instance of `IECore`.\n",
    "The class `IECore` has a special function called `load_network`, which loads a network to a device.\n",
    "This function prepares the network for the first inference on the device \n",
    "and returns an instance of the network prepared for an inference (execution). \n",
    "This function has many parameters, but in this case, you need to know only about two of them:\n",
    "* `network` - instance of `IENetwork`\n",
    "* `device_name` - string, contains a device name to infer a model on: CPU, GPU and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_recognition_network_loaded_on_device = ie_core.load_network(emotion_recognition_network, 'CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Open the input video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video_stream = cv2.VideoCapture(INPUT_VIDEO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Create an output video stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_video_stream = prapare_out_video_stream(input_video_stream, OUTPUT_VIDEO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input layer of the emotions-recognition-retail-0003 is data\n"
     ]
    }
   ],
   "source": [
    "emotion_recognition_input_layer = next(iter(emotion_recognition_network.input_info))\n",
    "emotion_recognition_input_blob = emotion_recognition_network.input_info[emotion_recognition_input_layer].input_data\n",
    "\n",
    "print(f'Input layer of the emotions-recognition-retail-0003 is {emotion_recognition_input_layer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape of the RetinaFace is [1, 3, 64, 64]\n"
     ]
    }
   ],
   "source": [
    "emotion_recognition_batch, emotion_recognition_channels, emotion_recognition_input_layer_h, emotion_recognition_input_layer_w = emotion_recognition_input_blob.shape\n",
    "\n",
    "print(f'Input shape of the RetinaFace is [{emotion_recognition_batch}, {emotion_recognition_channels}, {emotion_recognition_input_layer_h}, {emotion_recognition_input_layer_w}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_recognition_output_layer = next(iter(emotion_recognition_network.outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Prepare a frame and run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_infer(face):\n",
    "    # Resize the frame to the network input\n",
    "    resized_frame = cv2.resize(face, (emotion_recognition_input_layer_w, emotion_recognition_input_layer_h))\n",
    "    \n",
    "    # Change the data layout from HWC to CHW\n",
    "    transposed_frame = resized_frame.transpose((2, 0, 1))  \n",
    "    \n",
    "    # Reshape the frame to the network input \n",
    "    reshaped_frame = transposed_frame.reshape((emotion_recognition_batch, emotion_recognition_channels, emotion_recognition_input_layer_h, emotion_recognition_input_layer_w))\n",
    "\n",
    "    # Run the inference how you did it early\n",
    "    inference_results = emotion_recognition_network_loaded_on_device.infer({\n",
    "        emotion_recognition_input_layer: reshaped_frame\n",
    "    })\n",
    "    # For understanding what is the result of inference this model, check documentation \n",
    "    # https://docs.openvinotoolkit.org/latest/_models_intel_emotions_recognition_retail_0003_description_emotions_recognition_retail_0003.html\n",
    "    return inference_results[emotion_recognition_output_layer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 16: Drow boxes and emotions in a frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smile_by_index(emotion_inference_result: np.ndarray) -> np.ndarray:\n",
    "    emotions = ['neutral', 'happy', 'sad', 'surprise', 'anger']\n",
    "    emotion_index = np.argmax(emotion_inference_result.flatten()) \n",
    "    smile_path = f'./data/{emotions[emotion_index]}.png'\n",
    "    return cv2.imread(smile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_recognition_inference_postpprocess(original_frame, detected_face, emotion_result, x_limits, y_limits):\n",
    "    smile = get_smile_by_index(emotion_result)\n",
    "    # Put the title to a frame\n",
    "    w = x_limits[1] - x_limits[0]\n",
    "    h = y_limits[1] - y_limits[0]\n",
    "    print(w, h)\n",
    "    resized_smile = cv2.resize(smile, (w, h))\n",
    "    \n",
    "    original_frame[y_limits[0]:y_limits[1], x_limits[0]:x_limits[1]] = resized_smile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 17: Loop over frames in the input video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "435 571\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.3-openvino) ../opencv/modules/imgproc/src/resize.cpp:4051: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-0a467a6ac21d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Get height and width of the frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0memotion_recognition_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memotion_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memotion_recognition_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0memotion_recognition_inference_postpprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetected_face\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memotion_recognition_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mxmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mymin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mymax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Write the resulting frame to the output stream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-92-98051e5a7782>\u001b[0m in \u001b[0;36memotion_recognition_inference_postpprocess\u001b[0;34m(original_frame, detected_face, emotion_result, x_limits, y_limits)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_limits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_limits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mresized_smile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0moriginal_frame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_limits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my_limits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_limits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx_limits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresized_smile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.5.3-openvino) ../opencv/modules/imgproc/src/resize.cpp:4051: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"
     ]
    }
   ],
   "source": [
    "while input_video_stream.isOpened():\n",
    "    \n",
    "    # Read the next frame from the intput video \n",
    "    ret, original_frame = input_video_stream.read()\n",
    "    # Check if the video is over\n",
    "    if not ret:\n",
    "        # Exit from the loop if the video is over\n",
    "        break \n",
    "    face_detection_frame = face_detection_pre_processing(original_frame, face_detection_batch, face_detection_channels, face_detection_input_layer_h, face_detection_input_layer_w)\n",
    "    face_detection_inferece_result = face_detection_inference(face_detection_frame)\n",
    "    frame_h, frame_w = original_frame.shape[:2]\n",
    "    for detected_face in face_detection_inferece_result[0][0]:\n",
    "        if detected_face[2] < 0.5:\n",
    "            continue\n",
    "        # Step 13: Get the confidence for a discovered object\n",
    "        xmin = int(detected_face[3]*frame_w)\n",
    "        ymin = int(detected_face[4]*frame_h)\n",
    "\n",
    "        xmax = int(detected_face[5]*frame_w)\n",
    "        ymax = int(detected_face[6]*frame_h)\n",
    "        \n",
    "        emotion_recognition_frame = original_frame[ymin:ymax, xmin:xmax]\n",
    "    \n",
    "        # Get height and width of the frame\n",
    "        emotion_recognition_result = emotion_infer(emotion_recognition_frame)\n",
    "        emotion_recognition_inference_postpprocess(original_frame, detected_face, emotion_recognition_result, (xmin, xmax), (ymin, ymax))\n",
    "        # Write the resulting frame to the output stream\n",
    "    \n",
    "    output_video_stream.write(original_frame)\n",
    "    \n",
    "input_video_stream.release()\n",
    "# Save the resulting video\n",
    "output_video_stream.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the person (Artyom) on the resulting video will be detected with emotion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a source video\n",
    "HTML(f\"\"\"<video width=\"600\" height=\"400\" controls><source src=\"{OUTPUT_VIDEO}\" type=\"video/mp4\"></video>\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (OpenVINO 2021.4 LTS)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
